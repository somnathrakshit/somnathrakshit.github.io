<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.5.0"><meta name=author content="Somnath Rakshit"><meta name=description content="Description of our paper titled 'Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks', accepted at CHIIR 2020"><link rel=alternate hreflang=en-us href=https://somnathrakshit.github.io/post/relevance-prediction-eye-movement-cnn/><meta name=theme-color content=#2962ff><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/css/academic.min.04a87a1cb9027e3c50f566322527c56f.css><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-107696056-2','auto');ga('require','eventTracker');ga('require','outboundLinkTracker');ga('require','urlChangeTracker');ga('send','pageview');</script><script async src=https://www.google-analytics.com/analytics.js></script><script async src=https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin=anonymous></script><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/img/icon-32.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=https://somnathrakshit.github.io/post/relevance-prediction-eye-movement-cnn/><meta property=twitter:card content=summary_large_image><meta property=twitter:site content=@_SomnathRakshit><meta property=twitter:creator content=@_SomnathRakshit><meta property=og:site_name content="Somnath Rakshit"><meta property=og:url content=https://somnathrakshit.github.io/post/relevance-prediction-eye-movement-cnn/><meta property=og:title content="Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks | Somnath Rakshit"><meta property=og:description content="Description of our paper titled 'Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks', accepted at CHIIR 2020"><meta property=og:image content=https://somnathrakshit.github.io/post/relevance-prediction-eye-movement-cnn/featured.png><meta property=twitter:image content=https://somnathrakshit.github.io/post/relevance-prediction-eye-movement-cnn/featured.png><meta property=og:locale content=en-us><meta property=article:published_time content=2019-12-24T18:41:08-06:00><meta property=article:modified_time content=2019-12-24T18:41:08-06:00><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://somnathrakshit.github.io/post/relevance-prediction-eye-movement-cnn/"},"headline":"Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks","image":["https://somnathrakshit.github.io/post/relevance-prediction-eye-movement-cnn/featured.png"],"datePublished":"2019-12-24T18:41:08-06:00","dateModified":"2019-12-24T18:41:08-06:00","publisher":{"@type":"Organization","name":"Somnath Rakshit","logo":{"@type":"ImageObject","url":"https://somnathrakshit.github.io/img/icon-512.png"}},"description":"Description of our paper titled 'Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks', accepted at CHIIR 2020"}</script><title>Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks | Somnath Rakshit</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id=navbar-main><div class=container><a class=navbar-brand href=/>Somnath Rakshit</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/project><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/publication><span>Publications</span></a></li><li class=nav-item><a class="nav-link  active" href=/post><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/files/Somnath_Rakshit_Resume.pdf><span>Resume</span></a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class="nav-link js-search" href=#><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><article class=article><div class="article-container pt-3"><h1>Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks</h1><div class=article-metadata><div><span><a href=/authors/nilavra-bhattacharya/>Nilavra Bhattacharya</a></span>, <span><a href=/authors/admin/>Somnath Rakshit</a></span>, <span><a href=/authors/jacek-gwizdka/>Jacek Gwizdka</a></span>, <span><a href=/authors/paul-kogut/>Paul Kogut</a></span></div><span class=article-date>Dec 24, 2019</span>
<span class=middot-divider></span><span class=article-reading-time>9 min read</span><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://somnathrakshit.github.io/post/relevance-prediction-eye-movement-cnn/&amp;text=Relevance%20Prediction%20from%20Eye-movements%20Using%20Semi-interpretable%20Convolutional%20Neural%20Networks" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://somnathrakshit.github.io/post/relevance-prediction-eye-movement-cnn/&amp;t=Relevance%20Prediction%20from%20Eye-movements%20Using%20Semi-interpretable%20Convolutional%20Neural%20Networks" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook-f"></i></a></li><li><a href="mailto:?subject=Relevance%20Prediction%20from%20Eye-movements%20Using%20Semi-interpretable%20Convolutional%20Neural%20Networks&amp;body=https://somnathrakshit.github.io/post/relevance-prediction-eye-movement-cnn/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://somnathrakshit.github.io/post/relevance-prediction-eye-movement-cnn/&amp;title=Relevance%20Prediction%20from%20Eye-movements%20Using%20Semi-interpretable%20Convolutional%20Neural%20Networks" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=Relevance%20Prediction%20from%20Eye-movements%20Using%20Semi-interpretable%20Convolutional%20Neural%20Networks%20https://somnathrakshit.github.io/post/relevance-prediction-eye-movement-cnn/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://somnathrakshit.github.io/post/relevance-prediction-eye-movement-cnn/&amp;title=Relevance%20Prediction%20from%20Eye-movements%20Using%20Semi-interpretable%20Convolutional%20Neural%20Networks" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:630px;max-height:754px><div style=position:relative><img src=/post/relevance-prediction-eye-movement-cnn/featured.png alt class=featured-image></div></div><div class=article-container><div class=article-style><p>We propose an image-classification method to predict user&rsquo;s perceived-relevance from their eye-movement patterns. Our method is free from many of the inherent problems associated with analyzing eye-tracking data, as shown in existing literature. Specifically, we convert participant&rsquo;s eye-movement scanpaths into images, and then transform the relevance-prediction problem into an image-classification problem. For this purpose, we use state-of-the art image classifiers based on convolutional neural networks. Our method gives promising results, and outperforms many previously reported performances in similar studies by appreciable margins. We also attempt to interpret how the classifier possibly differentiates between user-reading-patterns on relevant and irrelevant documents. Finally, we discuss the limitations of our approach, and propose future directions of research.<h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#user-study>User Study</a><ul><li><a href=#experimental-design-and-procedure>Experimental Design and Procedure</a></li><li><a href=#stimuli-dataset>Stimuli Dataset</a></li></ul></li><li><a href=#data-analysis>Data Analysis</a><ul><li><a href=#generating-scanpath-images>Generating Scanpath Images</a><ul><li><a href=#fixations>Fixations</a></li><li><a href=#linearized-saccades>Linearized Saccades</a></li><li><a href=#colours>Colours</a></li></ul></li><li><a href=#machine-learning-setup>Machine Learning Setup</a><ul><li><a href=#train-validation-test-partition>Train / Validation / Test Partition</a></li></ul></li><li><a href=#analysis-procedure>Analysis Procedure</a><ul><li><a href=#image-classification-setup>Image Classification Setup</a></li><li><a href=#comparison-to-existing-standard>Comparison to Existing Standard</a></li></ul></li></ul></li><li><a href=#results-and-discussion>Results and Discussion</a><ul><li><a href=#scanpath-image-classification>Scanpath Image Classification</a></li></ul></li></ul></nav></p><h1 id=user-study>User Study</h1><h2 id=experimental-design-and-procedure>Experimental Design and Procedure</h2><p>A controlled lab experiment was conducted in the Department of Kinesiology, University of Maryland, College Park. Participants ($N=25$, college-age students) judged the relevance of short news articles for answering a trigger question. Eye-tracking and EEG signals were recorded.</p><figure><a data-fancybox href=img/procedure.png><img src=img/procedure.png alt></a><figcaption data-pre=Figure data-post=: class=numbered><h4>Experimental procedure. The &quot;+&quot; represents a fixation screen. The order of Relevant, Irrelevant and Topical articles were permuted in each trial</h4></figcaption></figure><p>The main element of the experimental procedure was a trial. In each trial, a trigger question was shown first. The trigger questions was a short, one-sentence question, informing participants what to look for in the subsequently presented documents (e.g. &quot;What is the birth name of Jesse Ventura?&quot;). After the trigger question, a short news article was displayed, then a text relevance response (Y/N) screen appeared, then a list of words for further assessment was shown. Participants progressed between stimuli by pressing a space bar, with an exception of moving from a news article to the text-relevance response screen, which occurred by participants fixating their eyes for two seconds or longer in the lower-right screen-corner to indicate their readiness for relevance judgement. Finally, a fixation screen was shown for one second between trials. The list of words for further assessment are not analysed in this paper. The news articles were chosen to have three levels of relevance with respect to the trigger question:</p><ol><li><strong>Relevant <code>(R)</code></strong>: the article explicitly contained the exact answer asked in the question.</li><li><strong>Topical <code>(T)</code></strong>: partially relevant – the article did not contain the exact answer to the question, but was on the topic of the information asked in the question.</li><li><strong>Irrelevant <code>(I)</code></strong>: did not contain the answer to the question</li></ol><h2 id=stimuli-dataset>Stimuli Dataset</h2><p>The set of 40 trigger questions were selected from the <code>TREC 2005 Question Answering Task</code>. The collection of 120 short news articles and their document-relevance labels came from the <code>AQUAINT Corpus of English News Text</code> (the same collection used in <code>TREC 2005 Q\&amp;A Task</code>. The news articles were carefully selected to have nearly similar text-length (<code>mean length</code>: 178 words, <code>SD</code>: 30 words.</p><h1 id=data-analysis>Data Analysis</h1><h2 id=generating-scanpath-images>Generating Scanpath Images</h2><p>We generated scanpath images from eye-tracking data of user-document pairs, using only three attributes of eye-fixations: screen-coordinates (in pixels), fixation duration (in ms), and start time of the fixation relative to stimulus-onset. We used Python Matplotlib library to generate the scanpath images. CNNs have been shown to be good at detecting local patterns within images. Since we were preparing the images for training a CNN classifier, we made the following design choices:</p><h3 id=fixations>Fixations</h3><p>Eye-fixations were encoded as marker points having varying shapes, sizes, and colours. These were controlled by the fixation duration as follows:</p><ol><li>110 - 250 ms: Level 1 fixations, encoded as red circle</li><li>250 - 400 ms: Level 2 fixations, encoded as pink star</li><li>400 - 550 ms: Level 3 fixations, encoded as yellow pentagon</li><li>&gt; 550 ms: Level 4 fixations, encoded as white cross</li></ol><p>These levels were identified empirically. We examined the distribution of fixation durations in our data, and roughly divided the range into three equal partitions. Fixations having durations less than 110 ms were discarded. The marker-size was made to increase with the Level number. The fixation markers were chosen to be grossly different from each other (instead of, say, only circles), so that the CNN could possibly identify spatial patterns of similar-duration fixations.</p><figure><a data-fancybox href=img/fixn_dur_hist.png><img src=img/fixn_dur_hist.png alt></a><figcaption data-pre=Figure data-post=: class=numbered><h4>Distribution of fixation-durations recorded in our study, and the corresponding encoding marker for representing fixations belonging to different levels, according to their duration</h4></figcaption></figure><h3 id=linearized-saccades>Linearized Saccades</h3><p>Saccades are rapid eye-movements between two fixation points. We controlled the colour of the saccade lines to follow a linear colour scale, based on their temporal occurrence <a href=https://matplotlib.org/3.1.1/gallery/color/colormap_reference.html target=_blank>(&quot;Winter&quot; colourmap in Matplotlib)</a>
The colour of the saccades changed linearly from blue (first saccade) to green (final saccade).
Each individual saccade had a solid colour.</p><p>We also tested controlling the width of the saccade lines using saccade velocity (ratio of screen-distance covered to time taken). However, doing so made the scanpath-image too crowded, especially for scanpaths having more than 50 fixations. So we kept the width of the saccade lines constant at 2 pixels.</p><h3 id=colours>Colours</h3><p>Care was taken to select the colours of the fixations and the saccades. Using a colour wheel, the colours of the different fixation markers were chosen to be far apart, from each other, as well as from the range of colours used to draw the saccades. We hypothesized that these colour choices would enable the CNN classifier to easily distinguish between fixations and saccades, and identify necessary patterns. Examples of typical eye-movement patterns on three types of documents, and their corresponding generated scanpath images are shown in Figure given below.</p><figure><a data-fancybox href=img/scanpath_encoding.png><img src=img/scanpath_encoding.png alt></a><figcaption data-pre=Figure data-post=: class=numbered><h4>Top: Typical eye-movement patterns when reading relevant, irrelevant, and topical documents. Bottom: Examples of generated scanpath images, which are used to train CNN classifiers for predicting the user’s perceived-relevance of the documents.</h4></figcaption></figure><h2 id=machine-learning-setup>Machine Learning Setup</h2><p>ata was available for 24 participants, where each participant judged the binary relevance of 120 news articles. In total we had eye-tracking data for 2,880 user-document pairs, or 2,880 scanpaths. After data cleaning, we decided to use scanpaths having 10 or more fixations. We assumed that at least 10 fixations, or a minimum eye dwell-time of 1 second on the document (at 100 ms / fixation) is required to make a relevance assessment. This left us with 2,579 scanpath images.</p><h3 id=train-validation-test-partition>Train / Validation / Test Partition</h3><p>We used the participants&rsquo; perceived-relevance labels as the ground-truth for our classification task (and not the document-relevance obtained from TREC dataset). Out of the 2,579 scanpath images, only 806 (31.2%) were for documents marked relevant. Thus, there was almost a 1:2 class imbalance. Since this is an initial attempt to apply image classification on scanpath images, we decided to use a balanced dataset. So we randomly sampled 806 images from the pool of irrelevant scanpath images, and created a perfectly balanced dataset of 1,612 images. We used an approximate 60-20-20 split to randomly place 966 images in the training set, 314 images in the validation set, and 332 images in the test set. The relevant/irrelevant class balance was preserved in each set. All random selections were performed using the MySQL <code>rand()</code> function.</p><h2 id=analysis-procedure>Analysis Procedure</h2><h3 id=image-classification-setup>Image Classification Setup</h3><p>We posed our binary classification problem as follows:
Given <strong>only</strong> the scanpath image of a user&rsquo;s eye movements on a short news article, did the user perceive the article to be relevant for answering a trigger question?</p><p>For this binary classification problem, we analysed the performance of six popular CNN based architectures: VGG16 and VGG19, DenseNet121 and DenseNet201, ResNet50, and InceptionResNet (version 2). All architectures had benchmark performances in the ImageNet challenge.</p><p>To examine whether the obtained results were reproducible in different environments and software versions, we ran the analyses independently on two popular Python deep-learning frameworks: <a href=https://www.tensorflow.org/guide/keras target=_blank>TensorFlow-Keras</a>, and <a href=https://docs.fast.ai/ target=_blank>PyTorch-fastai</a>.</p><figure><a data-fancybox href=img/flowchart.jpeg><img src=img/flowchart.jpeg alt></a><figcaption data-pre=Figure data-post=: class=numbered><h4>Architecture of the TensorFlow-Keras implementation. Optimizer: Stochastic Gradient Descent (SGD)</h4></figcaption></figure><p>In PyTorch-fastai, we built the classifier using the <code>cnn_learner</code> module, which initializes the model with random weights, and trains from scratch.</p><p>We trained the models on the training set, and used the validation set for very basic hyper-parameter tuning (learning rate, number of epochs, optimizer momentum, etc.). Since our intention was to see whether the method works, and not to obtain the best benchmark performance, we performed minimal hyper-parameter tuning. Finally, we took the best set of models obtained after tuning the hyper-parameters (<code>epochs</code>: <code>6</code>, <code>batch-size</code>: <code>16</code>, <code>momentum</code>: <code>0.9</code>)**, and used them to predict the labels of the test set. The top portion of Table reports the results from the TensorFlow-Keras implementation.</p><figure><a data-fancybox href=img/results_table.png><img src=img/results_table.png alt></a><figcaption data-pre=Figure data-post=: class=numbered><h4>Performances of two different methods to predict perceived-relevance from eye-movements, ordered by decreasing F1 score for the Test Set. Top: CNN classifiers on scanpath images. Bottom: traditional classifiers on aggregate features.</h4></figcaption></figure><h3 id=comparison-to-existing-standard>Comparison to Existing Standard</h3><p>We also compared our method to existing approaches for inferring relevance using eye-movements, where the data is collapsed into a set of handcrafted features. Perceived-relevance of documents are predicted from these features using popular classifiers like Random Forests and Support Vector Machines (SVM). We computed 20 such hand-engineered features, aggregated at the user-doc level, and classified them using Random Forest and SVM. This analysis was done using Python Scikit-learn library. Similar to our approach with the CNN classifiers, we started with the default hyperparameter values of the Random Forest and the SVM classifier from the Scikit-learn library, and then performed basic parameter tuning. Finally, we selected the best performing models. The bottom portion of Table given above reports these results.</p><h1 id=results-and-discussion>Results and Discussion</h1><h2 id=scanpath-image-classification>Scanpath Image Classification</h2><p>We report the performance of our proposed scanpath image classification method, by testing six different CNN classifier architectures. To easily compare our results to those reported in previous papers, we report five different metrics: percentages of correct predictions for both relevant and irrelevant documents, as True Positive Rate (TPR %) and True Negative Rate (TNR %); accuracy (Acc %); area under the ROC curve (ROC AUC); and F1-score (F1). We have ranked the image classifiers according to their F1 scores on the Test Set. We have taken care to report all range of performances &ndash; best, average, and worst &ndash; to provide realistic expectations of using this method. Using latest CNN classifiers, comparatively less training data, and leveraging the power of transfer-learning, it is possible to predict the perceived-relevance of documents from scanpath images with F1 score up to 0.81, and up to 80% accuracy.</p></div><div class=article-tags><a class="badge badge-light" href=/tags/computer-vision/>computer vision</a>
<a class="badge badge-light" href=/tags/deep-learning/>deep learning</a></div><div class="media author-card"><div class=media-body><h5 class=card-title><a href=/authors/nilavra-bhattacharya/></a></h5><ul class=network-icon aria-hidden=true></ul></div></div><div class=article-widget><div class=hr-light></div><h3>Related</h3><ul><li><a href=/publication/relevance-prediction-eye-movements-chiir-2020/>Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks</a></li><li><a href=/publication/chexray/>Deep Learning for Detection and Localization of Thoracic Diseases using Chest X-Ray Imagery</a></li><li><a href=/publication/potato-leaves-transfer-learning/>Detection of Diseases in Potato Leaves using Transfer Learning</a></li><li><a href=/project/neural-style-transfer/>Neural Style Transfer</a></li><li><a href=/project/fundus-image-localization/>Detection and Localisation of Diabetic Retinopathy</a></li></ul></div></div></article><script src=/js/mathjax-config.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js integrity="sha256-0w92bcB21IY5+rGI84MGj52jNfHNbXVeQLrZ0CGdjNY=" crossorigin=anonymous title=mermaid></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/python.min.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin=anonymous async></script><script>hljs.initHighlightingOnLoad();</script><script>const search_index_filename="/index.json";const i18n={'placeholder':"Search...",'results':"results found",'no_results':"No results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/academic.min.130521ecfc6f534c52c158217bbff718.js></script><div class=container><footer class=site-footer><p class=powered-by>&copy; Somnath Rakshit, 2020 &middot; .
<span class=float-right aria-hidden=true><a href=# id=back_to_top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&times;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>