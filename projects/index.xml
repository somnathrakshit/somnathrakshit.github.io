<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>List of projects on Somnath Rakshit</title><link>https://somnathrakshit.github.io/projects/</link><description>Recent content in List of projects on Somnath Rakshit</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Thu, 13 Aug 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://somnathrakshit.github.io/projects/index.xml" rel="self" type="application/rss+xml"/><item><title>Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks</title><link>https://somnathrakshit.github.io/projects/relevance-prediction-eye-movement-cnn/</link><pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate><guid>https://somnathrakshit.github.io/projects/relevance-prediction-eye-movement-cnn/</guid><description>Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks The primary purpose of Information Retrieval (IR) systems is to fetch content which is useful and relevant to people. IR systems have to cater to a variety of users, who may have wildly different mental models of what they consider to be useful and relevant.
Neuro-physiological methods, such as eye-tracking, provide an interesting avenue to observe users while they interact with information systems.</description></item><item><title>Answerability Classification Using Hand-Crafted Features</title><link>https://somnathrakshit.github.io/projects/project-answerability/</link><pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate><guid>https://somnathrakshit.github.io/projects/project-answerability/</guid><description>Answerability Classification Using Hand-Crafted Features In this project, I competed with the members in my class on a challenge to predict whether a visual question is answerable or not by using a given image and an associated question in the form of a text. For this task, we were required to create a multi-modal (computer vision + natural language processing) classification system.
First, Microsoft Azure Vision API was used to obtain the tags for each image.</description></item><item><title>geograpy3</title><link>https://somnathrakshit.github.io/projects/project-geograpy3/</link><pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate><guid>https://somnathrakshit.github.io/projects/project-geograpy3/</guid><description>geograpy3 geograpy3 extracts place names from a URL or text, and adds context to those names &amp;ndash; for example distinguishing between a country, region or city. It is a fork of Geograpy2, which is itself a fork of geograpy and inherits most of it, but solves several problems (such as support for utf8, places names with multiple words, confusion over homonyms etc). Also, geograpy3 is compatible with Python 3.6+, unlike Geography2.</description></item></channel></rss>