<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects on Somnath Rakshit</title><link>/projects/</link><description>Recent content in Projects on Somnath Rakshit</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 16 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="/projects/index.xml" rel="self" type="application/rss+xml"/><item><title>mri-sim-py.epg: A GPU-accelerated Extended Phase Graph Algorithm for differentiable optimization and learning</title><link>/projects/project-mri-sim-py-epg/</link><pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate><guid>/projects/project-mri-sim-py-epg/</guid><description>mri-sim-py.epg: A GPU-accelerated Extended Phase Graph Algorithm for differentiable optimization and learning Paper, Talk, Slides
The Extended Phase Graph Algorithm is a powerful tool for MRI sequence simulation and quantitative fitting, but such simulators are mostly written to run on CPU only and (with some exception) are poorly parallelized. A parallelized simulator compatible with other learning-based frameworks would be a useful tool to optimize scan parameters. Thus, we created an open source, GPU-accelerated EPG simulator in PyTorch.</description><content>&lt;h1 id="mri-sim-pyepg-a-gpu-accelerated-extended-phase-graph-algorithm-for-differentiable-optimization-and-learning">mri-sim-py.epg: A GPU-accelerated Extended Phase Graph Algorithm for differentiable optimization and learning&lt;/h1>
&lt;p>&lt;a href="/projects/project-mri-sim-py-epg/3754.html">Paper&lt;/a>, &lt;a href="https://youtu.be/EIexXEhh7Cc">Talk&lt;/a>, &lt;a href="https://docs.google.com/presentation/d/e/2PACX-1vThgT-tp8b3dKC6VU3Tn6A0JYhMW9mc7Gy0uzNGoq6sEsfHaGe2JBWcnOrOujtoqnEYiNUN_NTZvnMB/pub?start=true&amp;amp;loop=false&amp;amp;delayms=60000&amp;amp;slide=id.p1">Slides&lt;/a>&lt;br>
The Extended Phase Graph Algorithm is a powerful tool for MRI sequence simulation and quantitative fitting, but such simulators are mostly written to run on CPU only and (with some exception) are poorly parallelized. A parallelized simulator compatible with other learning-based frameworks would be a useful tool to optimize scan parameters. Thus, we created an open source, GPU-accelerated EPG simulator in PyTorch. Since the simulator is fully differentiable by means of automatic differentiation, it can be used to take derivatives with respect to sequence parameters, e.g. flip angles, as well as tissue parameters, e.g. T1 and T2.&lt;/p>
&lt;p>We chose mri-sim-py as the host package because it already contains different algorithms for simulating signals and this was a natural extension. Python was chosen as it can be demonstrated easily by means of Jupyter notebooks and interfaced with PyTorch, a popular machine learning auto-differentiation framework. The EPG Algorithm has three components: RF excitation, gradient application, and tissue relaxation.&lt;/p>
&lt;p>The latest version of this tool is available from the &lt;a href="https://github.com/utcsilab/mri-sim-py/tree/master/epg">Github repository&lt;/a> of mri-sim-py. Installation instructions are provided in the README file present there. Jupyter notebook containing an example is also available &lt;a href="https://github.com/utcsilab/mri-sim-py/blob/master/EPG_Parallel.ipynb">here&lt;/a>.&lt;/p>
&lt;p>After successfully parallelizing the simulator, we increased its speed by almost 100,000 times. This work was submitted as an abstract in ISMRM virtual conference.&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>This work was supported by the Amazon AWS Machine Learning Research grant.&lt;/p></content></item><item><title>Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks</title><link>/projects/relevance-prediction-eye-movement-cnn/</link><pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate><guid>/projects/relevance-prediction-eye-movement-cnn/</guid><description>Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks The primary purpose of Information Retrieval (IR) systems is to fetch content which is useful and relevant to people. IR systems have to cater to a variety of users, who may have wildly different mental models of what they consider to be useful and relevant.
Neuro-physiological methods, such as eye-tracking, provide an interesting avenue to observe users while they interact with information systems.</description><content>&lt;h1 id="relevance-prediction-from-eye-movements-using-semi-interpretable-convolutional-neural-networks">Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks&lt;/h1>
&lt;p>The primary purpose of Information Retrieval (IR) systems is to fetch content which is useful and relevant to people. IR systems have to cater to a variety of users, who may have wildly different mental models of what they consider to be useful and relevant.&lt;/p>
&lt;p>Neuro-physiological methods, such as eye-tracking, provide an interesting avenue to observe users while they interact with information systems. Eye-tracking has been frequently used to assess if the screen-content is relevant to the user. Despite its many advantages such as being non-invasive and requiring very little effort, interpreting eye-tracking data is not straightforward.&lt;/p>
&lt;p>For the dearth of standard methods, researchers resort to aggregating this data-stream into a set of single numbers, or features, at various levels of analysis (stimulus level, trial level, and/or participant level). By collapsing the eye-tracking data in this fashion, the fine grained information about the individual user&amp;rsquo;s progress is lost. This reduces the robustness and generalizability of insights gained from the analysis.&lt;/p>
&lt;blockquote>
&lt;p>So, how do we preserve the details about the user&amp;rsquo;s progress and ensure more robustness and generalizability?&lt;/p>
&lt;/blockquote>
&lt;p>We propose an image-classification method to predict user&amp;rsquo;s perceived-relevance from their eye-movement patterns. Specifically, we convert participant&amp;rsquo;s eye-movement scanpaths into images, and then transform the relevance-prediction problem into an image-classification problem. For this purpose, we use state-of-the art image classifiers based on convolutional neural networks. Our method gives promising results, and outperforms many previously reported performances in similar studies by appreciable margins. We also attempt to interpret how the classifier possibly differentiates between user-reading-patterns on relevant and irrelevant documents.&lt;/p>
&lt;figure class="left" >
&lt;img src="../../files/scanpath_encoding.png" />
&lt;/figure>
&lt;p>We generated scanpath images from eye-tracking data of user-document pairs, using only three attributes of eye-fixations: screen-coordinates (in pixels), fixation duration (in ms), and start time of the fixation relative to stimulus-onset.&lt;/p>
&lt;ol>
&lt;li>Eye-fixations were encoded as marker points having varying shapes, sizes, and colours. The marker-size was made to increase with the increase in fixation duration. The fixation markers were chosen to be grossly different from each other (instead of, say, only circles), so that the CNN could possibly identify spatial patterns of similar-duration fixations.&lt;/li>
&lt;li>We plotted linearized saccades: the effective eye-movement between two fixations, represented as a straight line connecting the two points. We controlled the colour of the saccade lines to follow a linear colour scale, based on their temporal occurrence. The colour of the saccades changed linearly from blue (first saccade) to green (final saccade).&lt;/li>
&lt;li>Using a colour wheel, the colours of the different fixation markers were chosen to be far apart, from each other, as well as from the range of colours used to draw the saccades.&lt;/li>
&lt;/ol>
&lt;p>We hypothesized that these colour choices would enable the CNN classifier to easily distinguish between fixations and saccades, and identify necessary patterns. Examples of typical eye-movement patterns on three types of documents, and their corresponding generated scanpath images are shown in Figure 1. One such plot is shown in Figure 2.&lt;/p>
&lt;figure class="left" >
&lt;img src="../../files/image.png" />
&lt;/figure>
&lt;p>Data was available for 24 participants, where each participant judged the binary relevance of 120 news articles. In total we had eye-tracking data for 2,880 user-document pairs, or 2,880 scanpaths.&lt;/p>
&lt;p>We posed our binary classification problem as follows:&lt;/p>
&lt;blockquote>
&lt;p>Given &lt;strong>only&lt;/strong> the scanpath image of a user&amp;rsquo;s eye movements on a short news article, did the user perceive the article to be relevant for answering a trigger question?&lt;/p>
&lt;/blockquote>
&lt;p>For this binary classification problem, we analysed the performance of six popular CNN based architectures: VGG16 and VGG19, DenseNet121 and DenseNet201, ResNet50 and InceptionResNet (version 2). The architecture was as below (Figure 3):&lt;/p>
&lt;p>
&lt;figure class="left" >
&lt;img src="../../files/flowchart.jpeg" />
&lt;/figure>
&lt;figure class="left" >
&lt;img src="../../files/results_table.png" />
&lt;/figure>
&lt;/p>
&lt;p>We trained the models on the training set, and used the validation set for very basic hyper-parameter tuning (learning rate, number of epochs, optimizer momentum, etc.). Since our intention was to see whether the method works, and not to obtain the best benchmark performance, we performed minimal hyper-parameter tuning. The top portion of the results table reports the results from the TensorFlow-Keras implementation.&lt;/p>
&lt;blockquote>
&lt;p>We thank Splunk Inc. for the &lt;a href="https://www.splunk.com/en_us/blog/security/deep-learning-with-splunk-and-tensorflow-for-security-catching-the-fraudster-in-neural-networks-with-behavioral-biometrics.html">blog post&lt;/a> on using mouse trajectories for fraud detection, which gave us the idea to adapt this approach for relevance prediction from eye-movements.&lt;/p>
&lt;/blockquote></content></item><item><title>Answerability Classification Using Hand-Crafted Features</title><link>/projects/project-answerability/</link><pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate><guid>/projects/project-answerability/</guid><description>Answerability Classification Using Hand-Crafted Features In this project, I competed with the members in my class on a challenge to predict whether a visual question is answerable or not by using a given image and an associated question in the form of a text. For this task, we were required to create a multi-modal (computer vision + natural language processing) classification system.
First, Microsoft Azure Vision API was used to obtain the tags for each image.</description><content>&lt;h1 id="answerability-classification-using-hand-crafted-features">Answerability Classification Using Hand-Crafted Features&lt;/h1>
&lt;p>In this project, I competed with the members in my class on a challenge to predict whether a visual question is answerable or not by using a given image and an associated question in the form of a text. For this task, we were required to create a multi-modal (computer vision + natural language processing) classification system.&lt;/p>
&lt;p>First, Microsoft Azure Vision API was used to obtain the tags for each image. These tags were then joined together with spaces in between to create artifical sentences. Then, the TF-IDF representation for these artificially generated sentences were created to be used later as features. The TF-IDF representation for the questions were also created as features.&lt;/p>
&lt;p>The two TF-IDF matrices were then stacked horizontally to create the unified feature set. We used this as the input dataset for our machine learning model, for which we selected random forest model using scikit-learn.&lt;/p>
&lt;p>However, we saw that the dataset was quite imbalanced. Thus, we tried to resample the dataset so that the minority class had equal number of samples as the majority class. We did this by randomly copying the samples from the minority class. Finally, we ran the random forest classifier on both the balanced as well as the imbalanced dataset. We found that the results improved substantially.&lt;/p>
&lt;p>Finally, we ran Grid Search to find the best hyperparameter for the random forest model and tested it on the validation set, which had not been resampled. We saw that when the number of estimators was equal to 1000, random forest model performed the best with respect to our testing metric, i.e. accuracy. This model was then submitted to run on the hidden test set. My model was then judged as the winner since it outperformed the models developed by all of my classmates.&lt;/p></content></item><item><title>geograpy3</title><link>/projects/project-geograpy3/</link><pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate><guid>/projects/project-geograpy3/</guid><description>geograpy3 geograpy3 extracts place names from a URL or text, and adds context to those names &amp;ndash; for example distinguishing between a country, region or city. It is a fork of Geograpy2, which is itself a fork of geograpy and inherits most of it, but solves several problems (such as support for utf8, places names with multiple words, confusion over homonyms etc). Also, geograpy3 is compatible with Python 3.6+, unlike Geography2.</description><content>&lt;h1 id="geograpy3">geograpy3&lt;/h1>
&lt;p>&lt;a href="https://github.com/somnathrakshit/geograpy3">geograpy3&lt;/a> extracts place names from a URL or text, and adds context to those names &amp;ndash; for example distinguishing between a country, region or city. It is a fork of &lt;a href="https://github.com/Corollarium/geograpy2">Geograpy2&lt;/a>, which is itself a fork of &lt;a href="https://github.com/ushahidi/geograpy">geograpy&lt;/a> and inherits most of it, but solves several problems (such as support for utf8, places names with multiple words, confusion over homonyms etc). Also, geograpy3 is compatible with Python 3.6+, unlike Geography2. This project is under active development by &lt;a href="https://github.com/somnathrakshit/geograpy3/graphs/contributors">these people&lt;/a>. This project has been downloaded over 35k times as of Nov 2020.&lt;/p>
&lt;p>This is installable from PyPi by running the following command.&lt;/p>
&lt;p>&lt;code>pip install geograpy3&lt;/code>&lt;/p></content></item></channel></rss>