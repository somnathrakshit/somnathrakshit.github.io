<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks</title><meta name=description content="Somnath Rakshit's personal website"><meta name=author content="Somnath Rakshit"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-107696056-2','auto');ga('send','pageview');}</script><link href="https://fonts.googleapis.com/css2?family=Oxygen:wght@400;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css integrity=sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2 crossorigin=anonymous><link rel=stylesheet href=/sass/researcher.min.css><link rel=icon type=image/ico href=https://somnath.pages.dev/favicon.ico></head><body><div class="container mt-5"><nav class="navbar navbar-expand-sm flex-column flex-sm-row p-0"><a class="navbar-brand mr-sm-auto" href=https://somnath.pages.dev/>Somnath Rakshit</a><div class="navbar-nav flex-row"><a class="nav-item nav-link" href=/>Home</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/files/somnathrakshit_resume.pdf>Resume</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/projects>Projects</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/musings>Musings</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/contact>Contact</a></div></nav></div><hr><div id=content><div class=container><h1 id=relevance-prediction-from-eye-movements-using-semi-interpretable-convolutional-neural-networks>Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks</h1><p>The primary purpose of Information Retrieval (IR) systems is to fetch content which is useful and relevant to people. IR systems have to cater to a variety of users, who may have wildly different mental models of what they consider to be useful and relevant.</p><p>Neuro-physiological methods, such as eye-tracking, provide an interesting avenue to observe users while they interact with information systems. Eye-tracking has been frequently used to assess if the screen-content is relevant to the user. Despite its many advantages such as being non-invasive and requiring very little effort, interpreting eye-tracking data is not straightforward.</p><p>For the dearth of standard methods, researchers resort to aggregating this data-stream into a set of single numbers, or features, at various levels of analysis (stimulus level, trial level, and/or participant level). By collapsing the eye-tracking data in this fashion, the fine grained information about the individual user&rsquo;s progress is lost. This reduces the robustness and generalizability of insights gained from the analysis.</p><blockquote><p>So, how do we preserve the details about the user&rsquo;s progress and ensure more robustness and generalizability?</p></blockquote><p>We propose an image-classification method to predict user&rsquo;s perceived-relevance from their eye-movement patterns. Specifically, we convert participant&rsquo;s eye-movement scanpaths into images, and then transform the relevance-prediction problem into an image-classification problem. For this purpose, we use state-of-the art image classifiers based on convolutional neural networks. Our method gives promising results, and outperforms many previously reported performances in similar studies by appreciable margins. We also attempt to interpret how the classifier possibly differentiates between user-reading-patterns on relevant and irrelevant documents.</p><figure><img src=/files/scanpath_encoding.png width=100%><figcaption><h4>Figure 1: Top: Typical eye-movement patterns when reading relevant, irrelevant, and topical documents. Bottom: Examples of generated scanpath images, which are used to train CNN classifiers for predicting the user's perceived-relevance of the documents.</h4></figcaption></figure><p>We generated scanpath images from eye-tracking data of user-document pairs, using only three attributes of eye-fixations: screen-coordinates (in pixels), fixation duration (in ms), and start time of the fixation relative to stimulus-onset.</p><ol><li>Eye-fixations were encoded as marker points having varying shapes, sizes, and colours. The marker-size was made to increase with the increase in fixation duration. The fixation markers were chosen to be grossly different from each other (instead of, say, only circles), so that the CNN could possibly identify spatial patterns of similar-duration fixations.</li><li>We plotted linearized saccades: the effective eye-movement between two fixations, represented as a straight line connecting the two points. We controlled the colour of the saccade lines to follow a linear colour scale, based on their temporal occurrence. The colour of the saccades changed linearly from blue (first saccade) to green (final saccade).</li><li>Using a colour wheel, the colours of the different fixation markers were chosen to be far apart, from each other, as well as from the range of colours used to draw the saccades.</li></ol><p>We hypothesized that these colour choices would enable the CNN classifier to easily distinguish between fixations and saccades, and identify necessary patterns. Examples of typical eye-movement patterns on three types of documents, and their corresponding generated scanpath images are shown in Figure 1. One such plot is shown in Figure 2.</p><figure><img src=/files/image.png width=100%><figcaption><h4>Figure 2: Plot obtained after converting the eye-tracking data into scanpath images using three attributes viz. screen-coordinates (in pixels), fixation duration (in ms), and start time of the fixation relative to stimulus-onset.</h4></figcaption></figure><p>Data was available for 24 participants, where each participant judged the binary relevance of 120 news articles. In total we had eye-tracking data for 2,880 user-document pairs, or 2,880 scanpaths.</p><p>We posed our binary classification problem as follows:</p><blockquote><p>Given <strong>only</strong> the scanpath image of a user&rsquo;s eye movements on a short news article, did the user perceive the article to be relevant for answering a trigger question?</p></blockquote><p>For this binary classification problem, we analysed the performance of six popular CNN based architectures: VGG16 and VGG19, DenseNet121 and DenseNet201, ResNet50 and InceptionResNet (version 2). The architecture was as below (Figure 3):</p><p><figure><img src=/files/flowchart.jpeg><figcaption><h4>Figure 3: Architecture of the TensorFlow-Keras implementation. Optimizer: Stochastic Gradient Descent (SGD)</h4></figcaption></figure><figure><img src=/files/results_table.png><figcaption><h4>Figure 4: Results obtained using different models</h4></figcaption></figure></p><p>We trained the models on the training set, and used the validation set for very basic hyper-parameter tuning (learning rate, number of epochs, optimizer momentum, etc.). Since our intention was to see whether the method works, and not to obtain the best benchmark performance, we performed minimal hyper-parameter tuning. The top portion of the results table reports the results from the TensorFlow-Keras implementation.</p><blockquote><p>We thank Splunk Inc. for the <a href=https://www.splunk.com/en_us/blog/security/deep-learning-with-splunk-and-tensorflow-for-security-catching-the-fraudster-in-neural-networks-with-behavioral-biometrics.html>blog post</a> on using mouse trajectories for fraud detection, which gave us the idea to adapt this approach for relevance prediction from eye-movements.</p></blockquote></div></div><div id=footer><hr><div class="container text-center mb-2"><a href=https://github.com/somnathrakshit><small>Copyright Somnath Rakshit, 2020</small></a></div></div></body></html>