<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>deep learning | Somnath Rakshit</title><link>https://somnathrakshit.github.io/tags/deep-learning/</link><atom:link href="https://somnathrakshit.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><description>deep learning</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Somnath Rakshit, 2020</copyright><lastBuildDate>Tue, 24 Dec 2019 18:41:08 -0600</lastBuildDate><image><url>https://somnathrakshit.github.io/img/icon-192.png</url><title>deep learning</title><link>https://somnathrakshit.github.io/tags/deep-learning/</link></image><item><title>Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks</title><link>https://somnathrakshit.github.io/post/relevance-prediction-eye-movement-cnn/</link><pubDate>Tue, 24 Dec 2019 18:41:08 -0600</pubDate><guid>https://somnathrakshit.github.io/post/relevance-prediction-eye-movement-cnn/</guid><description>&lt;blockquote&gt;
&lt;p&gt;We thank Splunk Inc. for the &lt;a href=&#34;https://www.splunk.com/en_us/blog/security/deep-learning-with-splunk-and-tensorflow-for-security-catching-the-fraudster-in-neural-networks-with-behavioral-biometrics.html&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt; on using mouse trajectories for fraud detection, which gave us the idea to adapt this approach for relevance prediction from eye-movements.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The primary purpose of Information Retrieval (IR) systems is to fetch content which is useful and relevant to people. IR systems have to cater to a variety of users, who may have wildly different mental models of what they consider to be useful and relevant.&lt;/p&gt;
&lt;p&gt;Neuro-physiological methods, such as eye-tracking, provide an interesting avenue to observe users while they interact with information systems. Eye-tracking has been frequently used to assess if the screen-content is relevant to the user. Despite its many advantages such as being non-invasive and requiring very little effort, interpreting eye-tracking data is not straightforward.&lt;/p&gt;
&lt;p&gt;For the dearth of standard methods, researchers resort to aggregating this data-stream into a set of single numbers, or features, at various levels of analysis (stimulus level, trial level, and/or participant level). By collapsing the eye-tracking data in this fashion, the fine grained information about the individual user&amp;rsquo;s progress is lost. This reduces the robustness and generalizability of insights gained from the analysis.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So, how do we preserve the details about the user&amp;rsquo;s progress and ensure more robustness and generalizability?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We propose an image-classification method to predict user&amp;rsquo;s perceived-relevance from their eye-movement patterns. Specifically, we convert participant&amp;rsquo;s eye-movement scanpaths into images, and then transform the relevance-prediction problem into an image-classification problem. For this purpose, we use state-of-the art image classifiers based on convolutional neural networks.
Our method gives promising results, and outperforms many previously reported performances in similar studies by appreciable margins. We also attempt to interpret how the classifier possibly differentiates between user-reading-patterns on relevant and irrelevant documents.&lt;/p&gt;
&lt;figure&gt;
&lt;a data-fancybox=&#34;&#34; href=&#34;img/scanpath_encoding.png&#34; &gt;
&lt;img src=&#34;img/scanpath_encoding.png&#34; alt=&#34;&#34; &gt;
&lt;/a&gt;
&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
&lt;h4&gt;Top: Typical eye-movement patterns when reading relevant, irrelevant, and topical documents. Bottom: Examples of generated scanpath images, which are used to train CNN classifiers for predicting the user&amp;rsquo;s perceived-relevance of the documents.&lt;/h4&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We generated scanpath images from eye-tracking data of user-document pairs, using only three attributes of eye-fixations: screen-coordinates (in pixels), fixation duration (in ms), and start time of the fixation relative to stimulus-onset.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Eye-fixations were encoded as marker points having varying shapes, sizes, and colours. The marker-size was made to increase with the increase in fixation duration. The fixation markers were chosen to be grossly different from each other (instead of, say, only circles), so that the CNN could possibly identify spatial patterns of similar-duration fixations.&lt;/li&gt;
&lt;li&gt;We plotted linearized saccades: the effective eye-movement between two fixations, represented as a straight line connecting the two points. We controlled the colour of the saccade lines to follow a linear colour scale, based on their temporal occurrence. The colour of the saccades changed linearly from blue (first saccade) to green (final saccade).&lt;/li&gt;
&lt;li&gt;Using a colour wheel, the colours of the different fixation markers were chosen to be far apart, from each other, as well as from the range of colours used to draw the saccades.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We hypothesized that these colour choices would enable the CNN classifier to easily distinguish between fixations and saccades, and identify necessary patterns. Examples of typical eye-movement patterns on three types of documents, and their corresponding generated scanpath images are shown in Figure 1. One such plot is shown in Figure 2.&lt;/p&gt;
&lt;figure&gt;
&lt;a data-fancybox=&#34;&#34; href=&#34;img/image.png&#34; &gt;
&lt;img src=&#34;img/image.png&#34; alt=&#34;&#34; &gt;
&lt;/a&gt;
&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
&lt;h4&gt;Plot obtained after converting the eye-tracking data into scanpath images using three attributes viz. screen-coordinates (in pixels), fixation duration (in ms), and start time of the fixation relative to stimulus-onset.&lt;/h4&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Data was available for 24 participants, where each participant judged the binary relevance of 120 news articles. In total we had eye-tracking data for 2,880 user-document pairs, or 2,880 scanpaths.&lt;/p&gt;
&lt;p&gt;We posed our binary classification problem as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Given &lt;strong&gt;only&lt;/strong&gt; the scanpath image of a user&amp;rsquo;s eye movements on a short news article, did the user perceive the article to be relevant for answering a trigger question?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For this binary classification problem, we analysed the performance of six popular CNN based architectures: VGG16 and VGG19, DenseNet121 and DenseNet201, ResNet50 and InceptionResNet (version 2). The architecture was as below (Figure 3):&lt;/p&gt;
&lt;figure&gt;
&lt;a data-fancybox=&#34;&#34; href=&#34;img/flowchart.jpeg&#34; &gt;
&lt;img src=&#34;img/flowchart.jpeg&#34; alt=&#34;&#34; &gt;
&lt;/a&gt;
&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
&lt;h4&gt;Architecture of the TensorFlow-Keras implementation. Optimizer: Stochastic Gradient Descent (SGD)&lt;/h4&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We trained the models on the training set, and used the validation set for very basic hyper-parameter tuning (learning rate, number of epochs, optimizer momentum, etc.). Since our intention was to see whether the method works, and not to obtain the best benchmark performance, we performed minimal hyper-parameter tuning. The top portion of Table~\ref{tab:results_table} reports the results from the TensorFlow-Keras implementation, while Table~\ref{tab:results_fastai} reports the results from the PyTorch-fastai implementation.&lt;/p&gt;
&lt;figure&gt;
&lt;a data-fancybox=&#34;&#34; href=&#34;img/flowchart.jpeg&#34; &gt;
&lt;img src=&#34;img/flowchart.jpeg&#34; alt=&#34;&#34; &gt;
&lt;/a&gt;
&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
&lt;h4&gt;Architecture of the TensorFlow-Keras implementation. Optimizer: Stochastic Gradient Descent (SGD)&lt;/h4&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;</description></item><item><title>Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks</title><link>https://somnathrakshit.github.io/publication/relevance-prediction-eye-movements-chiir-2020/</link><pubDate>Tue, 10 Dec 2019 16:16:12 -0600</pubDate><guid>https://somnathrakshit.github.io/publication/relevance-prediction-eye-movements-chiir-2020/</guid><description/></item><item><title>Deep Learning for Detection and Localization of Thoracic Diseases using Chest X-Ray Imagery</title><link>https://somnathrakshit.github.io/publication/chexray/</link><pubDate>Sat, 16 Mar 2019 01:59:10 +0200</pubDate><guid>https://somnathrakshit.github.io/publication/chexray/</guid><description/></item><item><title>Detection of Diseases in Potato Leaves using Transfer Learning</title><link>https://somnathrakshit.github.io/publication/potato-leaves-transfer-learning/</link><pubDate>Sun, 20 Jan 2019 14:21:24 +0530</pubDate><guid>https://somnathrakshit.github.io/publication/potato-leaves-transfer-learning/</guid><description>&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/somnathrakshit/22fcb7736e24254940b4216621781681.js&#34;&gt;&lt;/script&gt;</description></item><item><title>Deep Learning for Integrated Analysis of Breast Cancer Subtype Specific Multi-omics Data</title><link>https://somnathrakshit.github.io/publication/breast-cancer-integrated-sae/</link><pubDate>Wed, 31 Oct 2018 01:23:15 +0200</pubDate><guid>https://somnathrakshit.github.io/publication/breast-cancer-integrated-sae/</guid><description/></item><item><title>Neural Style Transfer</title><link>https://somnathrakshit.github.io/project/neural-style-transfer/</link><pubDate>Wed, 03 Oct 2018 02:32:45 +0530</pubDate><guid>https://somnathrakshit.github.io/project/neural-style-transfer/</guid><description>&lt;p&gt;Google Photos album containing the images is present &lt;a href=&#34;https://photos.app.goo.gl/GmeSraqqvKPxu7Zb6&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Implemented the neural style transfer algortihm implemented by Gatys et al. (2016) in PyTorch. This technique allows us to take an input image and perform its reproduction in a new artistic style. This algorithm takes two images as inputs. One is the style image that contains the artistic style whereas the other image contains the content which is to be modified in the given artistic style.&lt;/p&gt;</description></item><item><title>Detection and Localisation of Diabetic Retinopathy</title><link>https://somnathrakshit.github.io/project/fundus-image-localization/</link><pubDate>Tue, 15 May 2018 01:21:26 +0530</pubDate><guid>https://somnathrakshit.github.io/project/fundus-image-localization/</guid><description>&lt;p&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/e/2PACX-1vQpi7z7g1swVv-bcG2GeYggnhCjL1zXOq3BJYMXnqGtJC6pVg7Iq7qr-4bODfVGEcngX9PGiuJ5P8Qs/pub?start=false&amp;amp;loop=false&amp;amp;delayms=15000&#34; target=&#34;_blank&#34;&gt;Slides&lt;/a&gt;, &lt;a href=&#34;https://somnathrakshit.github.io/files/download/project_report_jgec.pdf&#34;&gt;Report&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Diabetic retinopathy occurs when the retina is damaged because fluids leak from blood vessels into the retina. The presence of hemorrhage is one of the earliest signs to indicate the severity of the disease. In this paper we review techniques, algorithms and methodologies used for the detection of hemorrhage from diabetic retinopathy retinal images. The retina is a transparent layer of vascularized neural tissue lining the inner layer of the back wall of the eye, between the retinal pigment epithelium on the outer and the vitreous on the inner side. The retina captures photons and converts these two photochemical and electrical energy integrates the Signals and transmits the resultant signal to the visual cortex of the brain via the optic nerve tracts and radiations. The retinal architecture is lamellar. Within this there are major types performing sensory nutritional regulatory immunomodulatory and structure and functions. The retina is uniquely partitioned from the vascular system by the blood retinal barrier and blood aqueous barrier. The blood supply is dual to the inner retina it is by the retinal circulation lying within the in a retina and to the outer retina it is by the choroidal circulation, Hrithik vascular layer lying outside of the retinal pigment epithelium. The retinal pigment epithelium and the choroid are critical to retinal function. Is the eye is imagined to be a camera in the retina is the film. Just like a picture cannot be developed if the camera has defective film, vision is not possible in an eye with a defective retina.&lt;/p&gt;
&lt;p&gt;If caught early then diabetic retinopathy can be treated, otherwise it can lead to irreversible blindness. Unluckily, medical specialists capable of detecting the disease are not available in many parts of the world where diabetes is prevalent. Deep neural networks have helped in solving many such problems in the recent past. Medical image classification has been done accurately using deep neural networks and thus, we will be using this in our project. The aim of this project is to use deep learning to help doctors identify the patients in need, particularly among underserved populations.&lt;/p&gt;
&lt;p&gt;Prolonged diabetes leads to DR, where the retina is damaged due to fluid leaking from the blood vessels. Usually, the stage of DR is judged based on blood vessels, exudes, hemorrhages, microaneurysms and texture. In this paper, we have discussed different methods for features extraction and automatic DR stage detection. An ophthalmologist uses an ophthalmoscope to visualize the blood vessels and his or her brain to detect the DR stages. Recently digital imaging became available as a tool for DR screening. It provides high quality permanent records of the retinal appearance, which can be used for monitoring of progression or response to treatment, and which can be reviewed by an ophthalmologist, digital images have the potential to be processed by automatic analysis systems. A combination of both accurate and early diagnosis as well as correct application of treatment can prevent blindness caused by DR in more than 50% of all cases. Therefore, regular screenings for DR of patients with diabetes is important. The grading of the resultant fundus images is an important cost factor. Automated DR detection can reduce the grading cost and thereby make the whole screening process less expensive. Some of the algorithms and systems reviewed in this paper are close to achieve DR identification in clinical practice.&lt;/p&gt;</description></item><item><title>Identifying Land Patterns from Satellite Imagery in Amazon Rainforest using Deep Learning</title><link>https://somnathrakshit.github.io/project/identify-land-pattern-keras/</link><pubDate>Tue, 27 Feb 2018 23:52:39 +0530</pubDate><guid>https://somnathrakshit.github.io/project/identify-land-pattern-keras/</guid><description>&lt;p&gt;The Amazon rainforests have been suffering widespread damage, both via natural and artificial means. Every minute, it is estimated that the world loses forest cover the size of 48 football fields. Deforestation in the Amazon rainforest has led to drastically reduced biodiversity, loss of habitat, climate change and other biological losses. In this respect, it has become essential to track how the nature of these forests change over time. Image classification using deep learning can help speed up this process by removing the manual task of classifying each image. Here, it is shown how convolutional neural networks can be used to track changes in land patterns in the Amazon rainforests. In this work, a testing accuracy of 96.71% was obtained. This can help governments and other agencies to track changes in land patterns more effectively and accurately.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://gist.github.com/somnathrakshit/334297f8ab77aa6739d97284d16addd9&#34; target=&#34;_blank&#34;&gt;Code,&lt;/a&gt; &lt;a href=&#34;https://somnathrakshit.github.io/files/download/ICNDE.pdf&#34;&gt;Poster&lt;/a&gt;.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/somnathrakshit/334297f8ab77aa6739d97284d16addd9.js&#34;&gt;&lt;/script&gt;</description></item></channel></rss>