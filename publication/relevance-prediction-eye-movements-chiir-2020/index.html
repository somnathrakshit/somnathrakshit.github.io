<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.5.0"><meta name=author content="Somnath Rakshit"><meta name=description content="We propose an image-classification method to predict the perceived-relevance of text documents from eye-movements. We conduct an eye-tracking study where participants read short news articles, and rate them as relevant or irrelevant for answering a trigger question. We encode participants' eye-movement scanpaths as images, and use these images to train a convolutional neural network classifier. The classifier is then used to predict the perceived-relevance of news article from the scanpath images. This method is content-independent, and the classifier does not require knowledge of the screen-content, or the participant's information-task. Even with little data, the image classifier can predict perceived-relevance with up to 80% accuracy. When compared to similar eye-tracking studies from the literature, the scanpath image classifier outperforms previously reported metrics by appreciable margins. We also attempt to interpret how the image classifier differentiates between scanpaths on relevant and irrelevant documents."><link rel=alternate hreflang=en-us href=https://somnathrakshit.github.io/publication/relevance-prediction-eye-movements-chiir-2020/><meta name=theme-color content=#2962ff><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/css/academic.min.04a87a1cb9027e3c50f566322527c56f.css><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-107696056-2','auto');ga('require','eventTracker');ga('require','outboundLinkTracker');ga('require','urlChangeTracker');ga('send','pageview');</script><script async src=https://www.google-analytics.com/analytics.js></script><script async src=https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin=anonymous></script><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/img/icon-32.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=https://somnathrakshit.github.io/publication/relevance-prediction-eye-movements-chiir-2020/><meta property=twitter:card content=summary_large_image><meta property=twitter:site content=@_SomnathRakshit><meta property=twitter:creator content=@_SomnathRakshit><meta property=og:site_name content="Somnath Rakshit"><meta property=og:url content=https://somnathrakshit.github.io/publication/relevance-prediction-eye-movements-chiir-2020/><meta property=og:title content="Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks | Somnath Rakshit"><meta property=og:description content="We propose an image-classification method to predict the perceived-relevance of text documents from eye-movements. We conduct an eye-tracking study where participants read short news articles, and rate them as relevant or irrelevant for answering a trigger question. We encode participants' eye-movement scanpaths as images, and use these images to train a convolutional neural network classifier. The classifier is then used to predict the perceived-relevance of news article from the scanpath images. This method is content-independent, and the classifier does not require knowledge of the screen-content, or the participant's information-task. Even with little data, the image classifier can predict perceived-relevance with up to 80% accuracy. When compared to similar eye-tracking studies from the literature, the scanpath image classifier outperforms previously reported metrics by appreciable margins. We also attempt to interpret how the image classifier differentiates between scanpaths on relevant and irrelevant documents."><meta property=og:image content=https://somnathrakshit.github.io/publication/relevance-prediction-eye-movements-chiir-2020/featured.png><meta property=twitter:image content=https://somnathrakshit.github.io/publication/relevance-prediction-eye-movements-chiir-2020/featured.png><meta property=og:locale content=en-us><meta property=article:published_time content=2019-12-10T16:16:12-06:00><meta property=article:modified_time content=2019-12-10T16:16:12-06:00><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://somnathrakshit.github.io/publication/relevance-prediction-eye-movements-chiir-2020/"},"headline":"Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks","image":["https://somnathrakshit.github.io/publication/relevance-prediction-eye-movements-chiir-2020/featured.png"],"datePublished":"2019-12-10T16:16:12-06:00","dateModified":"2019-12-10T16:16:12-06:00","publisher":{"@type":"Organization","name":"Somnath Rakshit","logo":{"@type":"ImageObject","url":"https://somnathrakshit.github.io/img/icon-512.png"}},"description":"We propose an image-classification method to predict the perceived-relevance of text documents from eye-movements. We conduct an eye-tracking study where participants read short news articles, and rate them as relevant or irrelevant for answering a trigger question. We encode participants' eye-movement scanpaths as images, and use these images to train a convolutional neural network classifier. The classifier is then used to predict the perceived-relevance of news article from the scanpath images. This method is content-independent, and the classifier does not require knowledge of the screen-content, or the participant's information-task. Even with little data, the image classifier can predict perceived-relevance with up to 80% accuracy. When compared to similar eye-tracking studies from the literature, the scanpath image classifier outperforms previously reported metrics by appreciable margins. We also attempt to interpret how the image classifier differentiates between scanpaths on relevant and irrelevant documents."}</script><title>Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks | Somnath Rakshit</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id=navbar-main><div class=container><a class=navbar-brand href=/>Somnath Rakshit</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/project><span>Projects</span></a></li><li class=nav-item><a class="nav-link  active" href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/post><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/files/Somnath_Rakshit_Resume.pdf><span>Resume</span></a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class="nav-link js-search" href=#><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><div class=pub><div class="article-container pt-3"><h1>Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks</h1><div class=article-metadata><div><span><a href=/authors/nilavra-bhattacharya/>Nilavra Bhattacharya</a></span>, <span><a href=/authors/admin/>Somnath Rakshit</a></span>, <span><a href=/authors/jacek-gwizdka/>Jacek Gwizdka</a></span></div><span class=article-date>December 2019</span><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://somnathrakshit.github.io/publication/relevance-prediction-eye-movements-chiir-2020/&amp;text=Relevance%20Prediction%20from%20Eye-movements%20Using%20Semi-interpretable%20Convolutional%20Neural%20Networks" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://somnathrakshit.github.io/publication/relevance-prediction-eye-movements-chiir-2020/&amp;t=Relevance%20Prediction%20from%20Eye-movements%20Using%20Semi-interpretable%20Convolutional%20Neural%20Networks" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook-f"></i></a></li><li><a href="mailto:?subject=Relevance%20Prediction%20from%20Eye-movements%20Using%20Semi-interpretable%20Convolutional%20Neural%20Networks&amp;body=https://somnathrakshit.github.io/publication/relevance-prediction-eye-movements-chiir-2020/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://somnathrakshit.github.io/publication/relevance-prediction-eye-movements-chiir-2020/&amp;title=Relevance%20Prediction%20from%20Eye-movements%20Using%20Semi-interpretable%20Convolutional%20Neural%20Networks" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=Relevance%20Prediction%20from%20Eye-movements%20Using%20Semi-interpretable%20Convolutional%20Neural%20Networks%20https://somnathrakshit.github.io/publication/relevance-prediction-eye-movements-chiir-2020/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://somnathrakshit.github.io/publication/relevance-prediction-eye-movements-chiir-2020/&amp;title=Relevance%20Prediction%20from%20Eye-movements%20Using%20Semi-interpretable%20Convolutional%20Neural%20Networks" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:592px><div style=position:relative><img src=/publication/relevance-prediction-eye-movements-chiir-2020/featured_hua74931088ea12a8c2e5d36784b4b8b76_678964_720x0_resize_lanczos_2.png alt class=featured-image>
<span class=article-header-caption>Class Activation Map showing the regions of interest as determined by our CNN</span></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>We propose an image-classification method to predict the perceived-relevance of text documents from eye-movements. We conduct an eye-tracking study where participants read short news articles, and rate them as relevant or irrelevant for answering a trigger question. We encode participants&rsquo; eye-movement scanpaths as images, and use these images to train a convolutional neural network classifier. The classifier is then used to predict the perceived-relevance of news article from the scanpath images. This method is content-independent, and the classifier does not require knowledge of the screen-content, or the participant&rsquo;s information-task. Even with little data, the image classifier can predict perceived-relevance with up to 80% accuracy. When compared to similar eye-tracking studies from the literature, the scanpath image classifier outperforms previously reported metrics by appreciable margins. We also attempt to interpret how the image classifier differentiates between scanpaths on relevant and irrelevant documents.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">Fifth ACM SIGIR Conference on Human Information Interaction and Retrieval</div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/tags/computer-vision/>computer vision</a>
<a class="badge badge-light" href=/tags/deep-learning/>deep learning</a></div><div class="media author-card"><div class=media-body><h5 class=card-title><a href=/authors/nilavra-bhattacharya/></a></h5><ul class=network-icon aria-hidden=true></ul></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/python.min.js></script><script>hljs.initHighlightingOnLoad();</script><script>const search_index_filename="/index.json";const i18n={'placeholder':"Search...",'results':"results found",'no_results':"No results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/academic.min.130521ecfc6f534c52c158217bbff718.js></script><div class=container><footer class=site-footer><p class=powered-by>&copy; Somnath Rakshit, 2019 &middot; .
<span class=float-right aria-hidden=true><a href=# id=back_to_top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&times;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>